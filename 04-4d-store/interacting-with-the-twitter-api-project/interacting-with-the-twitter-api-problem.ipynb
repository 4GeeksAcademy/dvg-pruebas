{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter can be used as a data source for various data science projects, including Geo-spatial analysis (where are users tweeting about certain subjects?) and sentiment analysis (how do users feel about certain subjects?).\n",
    "\n",
    "In this exercise we will learn how to stream real-time Twitter data. We will practice storing it in a dataframe to get some visualizations, as well as storing the data in a SQLite database, and building a web-app using Streamlit. Let's enumerate the tasks needed by dividing it into 3 areas:\n",
    "\n",
    "1. Database set-up: This can be done directly in the RDBMS of your choice, however we choose to use SQLite in this project.\n",
    "\n",
    "2. Tweepy: Credentials are required to interact with the Tweepy API. Once these have been obtained from dev.twitter.com we can set up a stream with keyword filters.\n",
    "\n",
    "3. Streamlit: Once we have our data stream working, we’ll need to set up our web app using Streamlit. This is surprisingly simple and can be done within a single python file!\n",
    "\n",
    "\n",
    "Tweepy is a Python library to access the Twitter API. You’ll need to set up a twitter application at dev.twitter.com to attain a set of authentication keys to use with the API. Streaming with Tweepy comprises of three objects; Stream, StreamListener, OAuthHandler. The latter simply handles API authentication and requires the unique keys from the creation of your Twitter app. As Tweepy has been updated last year, in order to avoid version conflicts and find more documentation, the streamlistener code will be provided to work with version v3.10.0 of Tweepy. In case you decide to use Tweepy v4.0.0 make sure to find the correct code for the StreamListener.\n",
    "\n",
    "**Good practice to include in your project**\n",
    "\n",
    "-Keep secrets and configuration out of version control**\n",
    "\n",
    "You really don't want to leak your Twitter secret key or database username and password on Github. Here's one way to do this, by storing your secrets and config variables in a special file (You learned it in the Cookiecutter template)\n",
    "\n",
    "Create a .env file in the project root folder. Thanks to the .gitignore, this file should never get committed into the version control repository. Here's an example:\n",
    "\n",
    "```py\n",
    "# example .env file\n",
    "DATABASE_URL=postgres://username:password@localhost:5432/dbname\n",
    "AWS_ACCESS_KEY=myaccesskey\n",
    "AWS_SECRET_ACCESS_KEY=mysecretkey\n",
    "OTHER_VARIABLE=something\n",
    "```\n",
    "\n",
    "-Using a package to load these variables automatically\n",
    "\n",
    "There is a package called python-dotenv to load up all the entries in this file as environment variables so they are accessible with os.environ.get. Here's an example snippet adapted from the python-dotenv documentation applied in the cookiecutter data science template:\n",
    "\n",
    "```py\n",
    "# src/data/dotenv_example.py\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# find .env automatically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "database_url = os.environ.get(\"DATABASE_URL\")\n",
    "other_variable = os.environ.get(\"OTHER_VARIABLE\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Storing in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import tweepy,json\n",
    "\n",
    "# Store OAuth authentication credentials in relevant variables\n",
    "\n",
    "access_token = ACCESS_TOKEN\n",
    "access_token_secret = ACCESS_TOKEN_SECRET\n",
    "consumer_key = CONSUMER_KEY\n",
    "consumer_secret = CONSUMER_SECRET\n",
    "\n",
    "# Pass OAuth details to tweepy's OAuth handler\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream import TweetListener\n",
    "\n",
    "# Initialize Stream listener\n",
    "l = TweetListener()\n",
    "\n",
    "# Create your Stream object with authentication\n",
    "stream = tweepy.Stream(auth, l)\n",
    "\n",
    "# Filter Twitter Streams to capture data by the keywords:\n",
    "stream.filter(['russia', 'ukraine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream import TweetListener\n",
    "\n",
    "# Initialize Stream listener\n",
    "l = TweetListener()\n",
    "\n",
    "# Create your Stream object with authentication\n",
    "stream = tweepy.Stream(auth, l)\n",
    "\n",
    "# Filter Twitter Streams to capture data by the keywords:\n",
    "stream.filter(['russia', 'ukraine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a dataframe with our Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizing some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store tweet counts\n",
    "[russia, ukraine] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    russia += word_in_text('russia', row['text'])\n",
    "    ukraine += word_in_text('ukraine', row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a list of labels:cd\n",
    "cd = ['russia', 'ukraine']\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = sns.barplot(cd, [russia, ukraine])\n",
    "ax.set(ylabel=\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Storing in a SQLite database\n",
    "\n",
    "The task here will be to apply your knowledge on creating a SQLite database and try reproduce the Streamlit app created in the following example:\n",
    "\n",
    "https://github.com/jonathanreadshaw/streamlit-twitter-stream/blob/master/stream.py\n",
    "\n",
    "Some explanation on the repo has been provided here: https://towardsdatascience.com/tracking-the-race-for-10-downing-street-live-tweet-dashboard-using-tweepy-mysql-and-streamlit-6084e88b4dd8#:~:text=The%20StreamListener%20class%20is%20used%20to%20define%20how,a%20new%20tweet%20is%20present%20in%20the%20stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. First let’s create our SQLite database.**\n",
    "\n",
    "Creation of the table and database transactions will be handled using SQLAlchemy. This is a Python library most commonly used as an Object Relational Mapper (ORM), handling the communication between our Python code and the database. SQLAlchemy allows database tables to be represented as Python classes and the use of functions to automatically execute SQL statements.\n",
    "\n",
    "The class below represents the table we will use to store tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models.py\n",
    "\n",
    "from sqlalchemy import Column, Integer, String, DateTime, Boolean, Float\n",
    "from database import Base\n",
    "\n",
    "\n",
    "class Tweet(Base):\n",
    "    __tablename__ = 'tweets'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    body = Column(String(1000), nullable=False)\n",
    "    keyword = Column(String(256), nullable=False)\n",
    "    tweet_date = Column(DateTime, nullable=False)\n",
    "    location = Column(String(100))\n",
    "    verified_user = Column(Boolean)\n",
    "    followers = Column(Integer)\n",
    "    sentiment = Column(Float)\n",
    "\n",
    "    def __init__(self, body, keyword, tweet_date, location, verified_user, followers, sentiment):\n",
    "        self.body = body\n",
    "        self.keyword = keyword\n",
    "        self.tweet_date = tweet_date\n",
    "        self.location = location\n",
    "        self.verified_user = verified_user\n",
    "        self.followers = followers\n",
    "        self.sentiment = sentiment\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Tweet %r>' % self.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a separate file database.py we define various variables that are required to perform our database operations using SQLAlchemy:\n",
    "\n",
    "Make sure to include the code to create the engine to your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#database.py\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import scoped_session, sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from config import DBConfig\n",
    "\n",
    "\n",
    "# CREATE ENGINE\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "\n",
    "Session = scoped_session(sessionmaker(autocommit=False, bind=engine))\n",
    "Base = declarative_base()\n",
    "\n",
    "@contextmanager\n",
    "def session_scope():\n",
    "    session = Session()\n",
    "    try:\n",
    "        yield session\n",
    "        session.commit()\n",
    "    except:\n",
    "        session.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "\n",
    "def init_db():\n",
    "    Base.metadata.create_all(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-session_scope(): this function uses the context managed decorator to provide a SQLAlchemy session object to perform transactions. The decorator allows for use within the with…as… syntax, and will commit or rollback depending on success, before closing the session.\n",
    "\n",
    "-init_db(): this function will create the tables defined by our SQLAlchemy models if they don’t exist.\n",
    "\n",
    "For more details you can go to the SQLAlchemy documentation.\n",
    "\n",
    "**2. Tweepy Stream**\n",
    "\n",
    "The StreamListener class is used to define how each incoming tweet should be handled. Again, make sure to see the explanation of each part of the StreamListener class in the example repo, here: https://towardsdatascience.com/tracking-the-race-for-10-downing-street-live-tweet-dashboard-using-tweepy-mysql-and-streamlit-6084e88b4dd8#:~:text=The%20StreamListener%20class%20is%20used%20to%20define%20how,a%20new%20tweet%20is%20present%20in%20the%20stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Creating a Streamlit app\n",
    "\n",
    "For your Streamlit web app you can take as a guide the following repo code: https://github.com/jonathanreadshaw/streamlit-twitter-stream/blob/master/stream.py\n",
    "\n",
    "Some ideas to include in your app (that have been included in the example repo):\n",
    "\n",
    "-Influential Tweets: Tweets from users with the largest number of followers.\n",
    "\n",
    "-Recent Tweets: The most recent tweets for the keywords in question.\n",
    "\n",
    "-Hourly/Daily volume by keyword\n",
    "\n",
    "-Hourly/Daily sentiment by keyword\n",
    "\n",
    "-Location of tweets\n",
    "\n",
    "These metrics/visualisations will allow users to keep track of trends in near real-time. We make use of the following streamlit features:\n",
    "\n",
    "-Caching: this is a key feature of streamlit. We can cache computationally expensive operation simply using the @st.cache decorator. These can be configured to expire after a set time (e.g. for loading fresh data)\n",
    "\n",
    "-Widgets: streamlit makes it easy to add interactivity to your app with buttons, drop downs etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: \n",
    "\n",
    "https://drivendata.github.io/cookiecutter-data-science/\n",
    "\n",
    "Repo example explanation: https://towardsdatascience.com/tracking-the-race-for-10-downing-street-live-tweet-dashboard-using-tweepy-mysql-and-streamlit-6084e88b4dd8#:~:text=The%20StreamListener%20class%20is%20used%20to%20define%20how,a%20new%20tweet%20is%20present%20in%20the%20stream.\n",
    "\n",
    "Integrating SQLite with SQLAlchemy: https://realpython.com/python-sqlite-sqlalchemy/\n",
    "\n",
    "Example repo: https://github.com/jonathanreadshaw/streamlit-twitter-stream/blob/master/stream.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
