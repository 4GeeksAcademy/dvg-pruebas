{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e902818",
   "metadata": {},
   "source": [
    "#  Raspado Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e28347",
   "metadata": {},
   "source": [
    "El Raspado Web es uno de los métodos importantes para recuperar datos de un sitio web automáticamente. No todos los sitios web permiten que las personas raspen, sin embargo, puede agregar 'robots.txt' después de la URL del sitio web que deseas raspar, para saber si se le permitirá raspar o no.\n",
    "\n",
    "¿Cómo obtenemos datos de un sitio web?\n",
    "\n",
    "Hay tres formas en las que podemos obtener datos de la web:\n",
    "\n",
    "1. Importando de archivos de Internet.\n",
    "\n",
    "2. Hacer Raspado Web directamente con código para descargar el contenido HMTL.\n",
    "\n",
    "3. Consultando datos de la API del sitio web.\n",
    "\n",
    "Pero, ¿qué es una API de sitio web?\n",
    "\n",
    "Una API (Interfaz de Programación de Aplicaciones) es una interfaz de software que permite que dos aplicaciones interactúen entre sí sin la intervención del usuario. Se puede acceder a una API web a través de la web utilizando el protocolo HTTP.\n",
    "\n",
    "Las herramientas de raspado son un software especialmente desarrollado para extraer datos de sitios web. ¿Cuáles son las herramientas más comunes para el Raspado Web?\n",
    "\n",
    "- Solicitudes: Es un módulo de Python en el que podemos enviar solicitudes HTTP para recuperar contenidos. Nos ayuda a acceder a los contenidos HTML o API del sitio web mediante el envío de solicitudes Get o Post.\n",
    "\n",
    "- Beautiful Soup: Nos ayuda a analizar documentos HTML o XML en un formato legible. Podemos recuperar información más rápido.\n",
    "\n",
    "- Selenio: se utiliza principalmente para pruebas de sitios web. Ayuda a automatizar diferentes eventos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023e1ac",
   "metadata": {},
   "source": [
    "### 1. Importación de archivos planos desde la web\n",
    "\n",
    "El archivo plano que importaremos será el dataset de iris de http://archive.ics.uci.edu/ml/machine-learning-databases/iris/ obtenido del repositorio de Machine Learning UCI.\n",
    "\n",
    "Después de importarlo, lo cargaremos en un dataframe (marco de datos) de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf2c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5.1,3.5,1.4,0.2,Iris-setosa\n",
      "0  4.9,3.0,1.4,0.2,Iris-setosa\n",
      "1  4.7,3.2,1.3,0.2,Iris-setosa\n",
      "2  4.6,3.1,1.5,0.2,Iris-setosa\n",
      "3  5.0,3.6,1.4,0.2,Iris-setosa\n",
      "4  5.4,3.9,1.7,0.4,Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Importar paquete\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Importar Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Asignar url de archivo: url\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "# Guardar archivo localmente\n",
    "urlretrieve(url,'iris.csv')\n",
    "\n",
    "# Leer el archivo en un dataframe y mirar las primeras filas\n",
    "df = pd.read_csv('iris.csv', sep=';')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43ce9f",
   "metadata": {},
   "source": [
    "### 2. Realización de solicitudes HTTP\n",
    "\n",
    "Pasos para hacer solicitudes HTTP:\n",
    "\n",
    "1. Inspeccionar el HTML del sitio web que queremos raspar (clic derecho).\n",
    "\n",
    "2. Acceder a la URL del sitio web usando código y descargar todo el contenido HTML en la página.\n",
    "\n",
    "3. Dar formato al contenido descargado en un formato legible.\n",
    "\n",
    "4. Extraer información útil y guardarla en un formato estructurado.\n",
    "\n",
    "5. Si la información se encuentra en varias páginas del sitio web, es posible que debamos repetir los pasos 2 a 4 para tener la información completa.\n",
    "\n",
    "**Realización de solicitudes HTTP utilizando urllib**\n",
    "\n",
    "Extraeremos el HTML en sí, pero primero empaquetaremos, enviaremos la solicitud y luego capturaremos la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77984bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar paquetes\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Especificar la URL\n",
    "url = \" https://scikit-learn.org/stable/getting_started.html\"\n",
    "\n",
    "# Esto empaqueta la solicitud\n",
    "request = Request(url)\n",
    "\n",
    "# Envíar la solicitud y capturar la respuesta\n",
    "response = urlopen(request)\n",
    "\n",
    "# Imprimir el tipo de datos de la respuesta\n",
    "print(type(response))\n",
    "\n",
    "# ¡Cierrar la respuesta!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ad6f5",
   "metadata": {},
   "source": [
    "Esta respuesta es un objeto http.client.HTTPResponse. ¿Qué podemos hacer con él?\n",
    "\n",
    "Como viene de una página HTML, podemos leerlo para extraer el HTML usando un método read() asociado a él. Ahora extraigamos la respuesta e imprimamos el HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9610a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request(url)\n",
    "\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extraer la respuesta: html\n",
    "html = response.read()\n",
    "\n",
    "# imprimir el html\n",
    "print(html)\n",
    "\n",
    "# ¡Cerrar la respuesta!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1401b",
   "metadata": {},
   "source": [
    "**Realizando solicitudes HTTP mediante solicitudes**\n",
    "\n",
    "Ahora vamos a usar la biblioteca de solicitudes. Esta vez no tenemos que cerrar la conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url = \"https://scikit-learn.org/stable/getting_started.html\"\n",
    "\n",
    "# Packages the request, send the request and catch the response: resp\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = resp.text\n",
    "\n",
    "# Print the html\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe2056",
   "metadata": {},
   "source": [
    "**Parsing HTML Using Beautiful Soup**\n",
    "\n",
    "We'll learn how to use the BeautifulSoup package to parse, prettify and extract information from HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://gvanrossum.github.io//'\n",
    "\n",
    "# Package the request, send the request and catch the response: resp\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = resp.text\n",
    "\n",
    "#Then, all we have to do is convert the HTML document to a BeautifulSoup object!\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa08e2",
   "metadata": {},
   "source": [
    "**Tags can be called in different ways**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2810657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line of code creates a BeautifulSoup object from a webpage:\n",
    " \n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    " \n",
    "# Within the `soup` object, tags can be called by name:\n",
    " \n",
    "first_div = soup.div\n",
    " \n",
    "# or by CSS selector:\n",
    " \n",
    "all_elements_of_header_class = soup.select(\".header\")\n",
    " \n",
    "# or by a call to `.find_all`:\n",
    " \n",
    "all_p_elements = soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd5f1b",
   "metadata": {},
   "source": [
    "### 3. Interacting with APIs\n",
    "\n",
    "it is a bit more complicated than scraping the HTML document, especially if authentication is required, but the data will be more structured and stable.\n",
    "\n",
    "Steps to querying data from the website API:\n",
    "\n",
    "1. Inspect the XHR network section of the URL that we want to scrape\n",
    "\n",
    "2. Find out the request-response that gives us the data that we want\n",
    "\n",
    "3. Depending on the type of request(post or get), let's simulate the request in our code and retrieve the data from API. If authentication is required, we will need to request for token first before sending our POST request\n",
    "\n",
    "4. Extract useful information that we need\n",
    "\n",
    "5. For API with a limit on query size, we will need to use ‘for loop’ to repeatedly retrieve all the data\n",
    "\n",
    "**Example: Loading and exploring a Json with GET request**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = \"https://covid-19-statistics.p.rapidapi.com/regions\"\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Host\": \"covid-19-statistics.p.rapidapi.com\",\n",
    "\t\"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = response.json()\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0b233",
   "metadata": {},
   "source": [
    "If you want to scrape a website, we should check the existence of API first in the network section using inspect. If we can find the response to a request that gives us all the data we need, we can build a stable solution. If we cannot find the data in-network, we can try using requests or Selenium to download HTML content and use Beautiful Soup to format the data.\n",
    "\n",
    "Other top web scraping tools in 2022:\n",
    "\n",
    "1. Newsdata.io\n",
    "\n",
    "2. Scrapingbee \n",
    "\n",
    "3. Bright Data\n",
    "\n",
    "4. Scraping-bot \n",
    "\n",
    "5. Scraper API \n",
    "\n",
    "6. Scrapestack \n",
    "\n",
    "7. Apify \n",
    "\n",
    "8. Agenty \n",
    "\n",
    "9. Import.io\n",
    "\n",
    "10. Outwit \n",
    "\n",
    "11. Webz.io "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945b8a8",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://towardsdatascience.com/web-scraping-basics-82f8b5acd45c\n",
    "\n",
    "https://rapidapi.com/rapidapi/api\n",
    "\n",
    "https://newsdata.io/blog/top-21-web-scraping-tools-for-you/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
