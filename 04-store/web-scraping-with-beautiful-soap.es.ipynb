{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e902818",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66e28347",
   "metadata": {},
   "source": [
    "Web Scraping se conoce como uno de los métodos más importantes para recuperar contenidos y datos de un sitio web automáticamente utilizando software. Esta información más tarde se puede utilizar para añadir contenido en una base de datos, exportar información a tipos de documentos estructurados, etcétera.\n",
    "\n",
    "El listado de lo que podemos \"scrapear\" en la web es amplio, pero incluye:\n",
    "- Redes sociales (Facebook, Twitter...).\n",
    "- Motores de búsqueda (Google, Bing...).\n",
    "- Páginas corporativas: tiendas online, servicios, de información empresarial, etcétera.\n",
    "- Páginas gubernamentales oficiales y de noticias.\n",
    "\n",
    "Existen dos formas de \"scrapear\", dependiendo de lo que queramos obtener de Internet:\n",
    "\n",
    "1. Obtener archivos/documentos.\n",
    "2. Obtener información.\n",
    "\n",
    "La diferencia entre el primer y segundo punto es que un archivo contiene información pero no está descrito en la página web. Con el segundo punto lo que buscamos es extraer párrafos, títulos, cantidades, importes, etcétera inmersos en la web.\n",
    "\n",
    "Como es evidente, utilizaremos `Python` para obtener contenido de Internet. Manteniendo el uso del mismo lenguaje aseguramos que todo el proceso de `ETL` quede integrado aumentando legibilidad y mantenibilidad."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5023e1ac",
   "metadata": {},
   "source": [
    "### 1. Obtener archivos/documentos\n",
    "\n",
    "En Python, el paquete `requests` permite interactuar con URIs HTTP y que posibilita, por ejemplo, descargar recursos y archivos alojados en alguna página web. La función que permite hacer esto es `get` y, en nuestro caso, permitiría descargar cierta información y transformarla, por ejemplo, en un DataFrame de Pandas.\n",
    "\n",
    "#### Paso 1. Encontrar el recurso a descargar\n",
    "En este caso estamos interesados en descargar información sobre los ingresos en Estados Unidos. Para ello, dado que no disponemos de información en nuestra base de datos (está vacía) buscamos recursos en Internet. Localizamos una fuente que nos podría permitir desarrollar un modelo predictivo, y accedemos a ella:\n",
    "\n",
    "![Scraping files step 1](https://github.com/4GeeksAcademy/machine-learning-content/tree/master/assets/scraping_web_files_step1.png?raw=true)\n",
    "\n",
    "#### Paso 2. Localizar el punto de descarga del recurso\n",
    "El siguiente paso es localizar desde qué dirección podremos descargar el recurso. El UCI repository proporciona una interfaz muy intuitiva para descargar recursos. Copiando la dirección del botón `Download` podremos obtener fácilmente el punto de descarga. Sin embargo, dependiendo de la página web a veces obtener este enlace es lo más complicado de todo el proceso.\n",
    "\n",
    "Tras analizar la web, obtenemos que el enlace de descarga es el siguiente: `https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data`\n",
    "\n",
    "![Scraping files step 2](https://github.com/4GeeksAcademy/machine-learning-content/tree/master/assets/scraping_web_files_step2.png?raw=true)\n",
    "\n",
    "#### Paso 3. Programar la descarga del fichero\n",
    "Lo último que queda antes de poder trabajar con la información es descargarla. Para ello utilizaremos el paquete `requests` ya que porporciona un mecanismo muy sencillo de utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Seleccionar el recurso a descargar\n",
    "resource_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "# Petición para descargar el fichero de Internet\n",
    "response = requests.get(resource_url)\n",
    "\n",
    "# Si la petición se ha ejecutado correctamente (código 200), entonces el fichero se ha podido descargar\n",
    "if response:\n",
    "    # Se almacena el archivo en el directorio actual para usarlo más tarde\n",
    "    with open(\"adult.csv\", \"wb\") as dataset:\n",
    "        dataset.write(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfdf6df3",
   "metadata": {},
   "source": [
    "El resultado es un fichero totalmente utilizable en nuestro directorio y que proviene de Internet, totalmente utilizable para el resto de pasos a realizar para entrenar nuestro modelo de Machine Learning.\n",
    "\n",
    "![Scraping files step 3](https://github.com/4GeeksAcademy/machine-learning-content/tree/master/assets/scraping_web_files_step3.png?raw=true)\n",
    "\n",
    "Ahora, podríamos leerlo con `Pandas` y crear un DataFrame a partir del fichero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63aec32e",
   "metadata": {},
   "source": [
    "### 2. Obtener información\n",
    "\n",
    "En Python, el paquete `requests` permite interactuar con URIs HTTP y que posibilita, por ejemplo, descargar recursos y archivos alojados en alguna página web. La función que permite hacer esto es `get` y, en nuestro caso, permitiría descargar cierta información y transformarla, por ejemplo, en un DataFrame de Pandas.\n",
    "\n",
    "#### Paso 1. Encontrar el recurso a descargar\n",
    "En este caso estamos interesados en descargar información sobre los ingresos en Estados Unidos. Para ello, dado que no disponemos de información en nuestra base de datos (está vacía) buscamos recursos en Internet. Localizamos una fuente que nos podría permitir desarrollar un modelo predictivo, y accedemos a ella:\n",
    "\n",
    "![Scraping files step 1](https://github.com/4GeeksAcademy/machine-learning-content/tree/master/assets/scraping_web_files_step1.png?raw=true)\n",
    "\n",
    "#### Paso 2. Localizar el punto de descarga del recurso\n",
    "El siguiente paso es localizar desde qué dirección podremos descargar el recurso. El UCI repository proporciona una interfaz muy intuitiva para descargar recursos. Copiando la dirección del botón `Download` podremos obtener fácilmente el punto de descarga. Sin embargo, dependiendo de la página web a veces obtener este enlace es lo más complicado de todo el proceso.\n",
    "\n",
    "Tras analizar la web, obtenemos que el enlace de descarga es el siguiente: `https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data`\n",
    "\n",
    "![Scraping files step 2](https://github.com/4GeeksAcademy/machine-learning-content/tree/master/assets/scraping_web_files_step2.png?raw=true)\n",
    "\n",
    "#### Paso 3. Programar la descarga del fichero\n",
    "Lo último que queda antes de poder trabajar con la información es descargarla. Para ello utilizaremos el paquete `requests` ya que porporciona un mecanismo muy sencillo de utilizar:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35580d2b",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd43ce9f",
   "metadata": {},
   "source": [
    "### 2. Realización de solicitudes HTTP\n",
    "\n",
    "Pasos para hacer solicitudes HTTP:\n",
    "\n",
    "1. Inspeccionar el HTML del sitio web que queremos raspar (clic derecho).\n",
    "\n",
    "2. Acceder a la URL del sitio web usando código y descargar todo el contenido HTML en la página.\n",
    "\n",
    "3. Dar formato al contenido descargado en un formato legible.\n",
    "\n",
    "4. Extraer información útil y guardarla en un formato estructurado.\n",
    "\n",
    "5. Si la información se encuentra en varias páginas del sitio web, es posible que debamos repetir los pasos 2 a 4 para tener la información completa.\n",
    "\n",
    "**Realización de solicitudes HTTP utilizando urllib**\n",
    "\n",
    "Extraeremos el HTML en sí, pero primero empaquetaremos, enviaremos la solicitud y luego capturaremos la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77984bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar paquetes\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Especificar la URL\n",
    "url = \" https://scikit-learn.org/stable/getting_started.html\"\n",
    "\n",
    "# Esto empaqueta la solicitud\n",
    "request = Request(url)\n",
    "\n",
    "# Envíar la solicitud y capturar la respuesta\n",
    "response = urlopen(request)\n",
    "\n",
    "# Imprimir el tipo de datos de la respuesta\n",
    "print(type(response))\n",
    "\n",
    "# ¡Cierrar la respuesta!\n",
    "response.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf3ad6f5",
   "metadata": {},
   "source": [
    "Esta respuesta es un objeto http.client.HTTPResponse. ¿Qué podemos hacer con él?\n",
    "\n",
    "Como viene de una página HTML, podemos leerlo para extraer el HTML usando un método read() asociado a él. Ahora extraigamos la respuesta e imprimamos el HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9610a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request(url)\n",
    "\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extraer la respuesta: html\n",
    "html = response.read()\n",
    "\n",
    "# imprimir el html\n",
    "print(html)\n",
    "\n",
    "# ¡Cerrar la respuesta!\n",
    "response.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69d1401b",
   "metadata": {},
   "source": [
    "**Realizando solicitudes HTTP mediante solicitudes**\n",
    "\n",
    "Ahora vamos a usar la biblioteca de solicitudes. Esta vez no tenemos que cerrar la conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Especificar la url: url\n",
    "url = \"https://scikit-learn.org/stable/getting_started.html\"\n",
    "\n",
    "# Empaquetar la solicitud, envíar la solicitud y capturar la respuesta: resp\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Extraer la respuesta: texto\n",
    "text = resp.text\n",
    "\n",
    "# imprimir el html\n",
    "print(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4fe2056",
   "metadata": {},
   "source": [
    "**Analizando HTML usando Beautiful Soup**\n",
    "\n",
    "Aprenderemos a usar el paquete BeautifulSoup para analizar, embellecer y extraer información de HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar paquetes\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Especificar la url: url\n",
    "url = 'https://gvanrossum.github.io//'\n",
    "\n",
    "# Empaquetar la solicitud, enviar la solicitud y obtener la respuesta: resp\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Extraer la respuesta como html: html_doc\n",
    "html_doc = resp.text\n",
    "\n",
    "# Luego, ¡todo lo que tenemos que hacer es convertir el documento HTML en un objeto BeautifulSoup!\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Embellecer el objeto BeautifulSoup: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(pretty_soup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddfa08e2",
   "metadata": {},
   "source": [
    "**Los tags se pueden llamar de diferentes maneras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2810657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta línea de código crea un objeto BeautifulSoup desde una página web:\n",
    " \n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    " \n",
    "# Dentro del objeto `sopa`, los tags se pueden llamar por su nombre:\n",
    " \n",
    "first_div = soup.div\n",
    " \n",
    "# O por selector de CSS:\n",
    " \n",
    "all_elements_of_header_class = soup.select(\".header\")\n",
    " \n",
    "# O por una llamada a `.find_all`:\n",
    " \n",
    "all_p_elements = soup.find_all(\"p\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67fd5f1b",
   "metadata": {},
   "source": [
    "### 3. Interactuando con APIs\n",
    "\n",
    "Es un poco más complicado que raspar el documento HTML, especialmente si se requiere autenticación, pero los datos serán más estructurados y estables.\n",
    "\n",
    "Pasos para consultar datos de la API del sitio web:\n",
    "\n",
    "1. Inspeccionar la sección de red XHR de la URL que queremos raspar.\n",
    "\n",
    "2. Averiguar la petición-respuesta que nos da los datos que queremos.\n",
    "\n",
    "3. Dependiendo del tipo de solicitud (publicar u obtener), simulemos la solicitud en nuestro código y recuperemos los datos de la API. Si se requiere autenticación, primero deberemos solicitar el token antes de enviar nuestra solicitud POST.\n",
    "\n",
    "4. Extraer información útil que necesitamos.\n",
    "\n",
    "5. Para API con un límite en el tamaño de la consulta, necesitaremos usar 'for loop' para recuperar repetidamente todos los datos\n",
    "\n",
    "**Ejemplo: cargando y explorando un Json con solicitud GET**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar paquete\n",
    "import requests\n",
    "\n",
    "# Asignar url a la variable: url\n",
    "url = \"https://covid-19-statistics.p.rapidapi.com/regions\"\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Host\": \"covid-19-statistics.p.rapidapi.com\",\n",
    "\t\"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "# Decodificar los datos JSON en un diccionario: json_data\n",
    "json_data = response.json()\n",
    "\n",
    "# Imprimir cada par clave-valor en json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92a0b233",
   "metadata": {},
   "source": [
    "Si deseas raspar un sitio web, primero debemos verificar la existencia de API en la sección de red usando inspeccionar. Si podemos encontrar la respuesta a una solicitud que nos proporcione todos los datos que necesitamos, podemos construir una solución estable. Si no podemos encontrar los datos en la red, podemos intentar usar solicitudes o Selenium para descargar contenido HTML y usar Beautiful Soup para formatear los datos.\n",
    "\n",
    "Otras herramientas principales de Raspado Web en 2022:\n",
    "\n",
    "1. Newsdata.io\n",
    "\n",
    "2. Scrapingbee \n",
    "\n",
    "3. Bright Data\n",
    "\n",
    "4. Scraping-bot \n",
    "\n",
    "5. Scraper API \n",
    "\n",
    "6. Scrapestack \n",
    "\n",
    "7. Apify \n",
    "\n",
    "8. Agenty \n",
    "\n",
    "9. Import.io\n",
    "\n",
    "10. Outwit \n",
    "\n",
    "11. Webz.io "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1945b8a8",
   "metadata": {},
   "source": [
    "Referencias: \n",
    "\n",
    "https://towardsdatascience.com/web-scraping-basics-82f8b5acd45c\n",
    "\n",
    "https://rapidapi.com/rapidapi/api\n",
    "\n",
    "https://newsdata.io/blog/top-21-web-scraping-tools-for-you/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
