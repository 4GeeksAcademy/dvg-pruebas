{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410097fb",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc37668",
   "metadata": {},
   "source": [
    "Feature engineering is an essential part of building any intelligent system. This is the reason Data Scientists often spend 70% of their time in the data preparation phase before modeling.\n",
    "\n",
    "**What is a feature?**\n",
    "\n",
    "A feature is typically a specific representation on top of raw data, which is an individual, measurable attribute, typically depicted by a column in a dataset. Considering a generic two-dimensional dataset, each observation is depicted by a row and each feature by a column, which will have a specific value for an observation.\n",
    "\n",
    "Features can be of two major types based on the dataset. Inherent raw features are obtained directly from the dataset with no extra data manipulation or engineering. Derived features are usually obtained from feature engineering, where we extract features from existing data attributes. A simple example would be creating a new feature “BMI” from an employee dataset containing “Weight” and \"Height\" by just using the formula with weight and height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b1dd5",
   "metadata": {},
   "source": [
    "### Feature Engineering on numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b018ed",
   "metadata": {},
   "source": [
    "Even though numeric data can be directly fed into machine learning models, we still need to engineer features which are relevant to the problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5b25e",
   "metadata": {},
   "source": [
    "### Feature engineering on categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7e975",
   "metadata": {},
   "source": [
    "### What are some naive feature engineering techniques that improve model efficacy?\n",
    "\n",
    "1. Summary statistics (mean, median, mode, min, max, std) for each group of similar records. For example, all female customers between the age 34 and 45 would get their own set of summary statistics.\n",
    "\n",
    "2. Interactions or ratios between features. For example, var1 * var2.\n",
    "\n",
    "3. Summaries of features. For example, the number of purchases a customer made in the last 30 days (raw features may be last 10 purchase dates).\n",
    "\n",
    "4. Splitting feature information manually. For example, customers taller than 1.80cm would be a critical piece of information when recommending car vs SUV.\n",
    "\n",
    "5. KNN using records in the training set to produce a KNN feature that is fed into another model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1a749",
   "metadata": {},
   "source": [
    "Source:\n",
    "\n",
    "https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b\n",
    "\n",
    "https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63\n",
    "\n",
    "https://medium.com/@mxcsyounes/hands-on-with-feature-engineering-techniques-broad-introduction-def389c1fc25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
