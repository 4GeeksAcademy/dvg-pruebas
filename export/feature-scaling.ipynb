{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1db469",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed536664",
   "metadata": {},
   "source": [
    "Following the data pre-processing steps, data scaling is a method of standardization that’s useful when working with a dataset that contains continuous features that are on different scales, and you’re using a model that operates in some sort of linear space (like linear regression or K-nearest neighbors).\n",
    "\n",
    "What does it mean with different scales?\n",
    "\n",
    "Most of the times, our dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem. If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n",
    "\n",
    "\n",
    "So imagine we’re looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if we don’t scale our prices, algorithms like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar!\n",
    "\n",
    "Remember, scaling means transforming the data so that it fits within a specific scale, like 0-100 or 0-1. Usually 0-1. We want to scale data especially when we’re using methods based on measures of how far apart data points are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2900233",
   "metadata": {},
   "source": [
    "### Methods for scaling the data\n",
    "\n",
    "1. Normalization or scaling\n",
    "\n",
    "General terms that refer to transforming our input data into a new scale. This distribution will have values between -1 and 1with μ=0.\n",
    "\n",
    "**Drawback:** It is sensitive to outliers since the presence of outliers  will compress most values and make them appear extremely close together.\n",
    "\n",
    "2. Min-Max\n",
    "\n",
    "Linear transformation of data that maps the minimum value to 0 and the maximum value to 1.\n",
    "\n",
    "**Drawback:** It is also sensitive to outliers sice the presence will compress most values and make them appear extremely close together.\n",
    "\n",
    "3. Standarization (or Z-score transformation)\n",
    "\n",
    "It transforms each feature to a normal distribution with a mean of 0 and standard deviation of 1. It replaces the values by their z-score.\n",
    "In python sklearn.preprocessing.scale helps us implementing standardisation.\n",
    "\n",
    "**Drawback:** It rescales to an unbounded interval which can be problematic for certain algorithms. For example, some neural networks that expect input values to be inside a specific range.\n",
    "\n",
    "Standardisation and Mean Normalization can be used for algorithms that assumes zero centric data like Principal Component Analysis(PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975bef33",
   "metadata": {},
   "source": [
    "### When should we scale the data? Why?\n",
    "\n",
    "When our algorithm will weight each input. For example, gradient descent used by many neural nets, or use distance metrics like KNN. \n",
    "Model performance can often be improved by normalizing, standarizing, or otherwise scaling the data so that each feature is given relatively equal weight.\n",
    "\n",
    "It is also important when features are measured in different units. For example, feature A is measured in inches, feature B is measured in feet and feature C is measured in dollars, that they are scaled in a way that they are weighted and/or represented equally.\n",
    "\n",
    "In some cases, efficacy will not change but perceived feature importance may change. For example, the coefficients in a linear regression.\n",
    "\n",
    "Scaling our data tipically does not change performance or feature importance for tree-based models since the split points will simply shift to compensate for the scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86554787",
   "metadata": {},
   "source": [
    "**Some algorithms where feature scaling matters:**\n",
    "\n",
    "-K-nearest neighbors: should be scaled for all features to weigh in equally.\n",
    "\n",
    "-Principal Component Analysis \n",
    "\n",
    "-Gradient Descent\n",
    "\n",
    "-Tree based models are not distance based models and can handle varying ranges of features. Scaling is not required while modelling trees.\n",
    "\n",
    "-Naive Bayes and Linear Discriminant Analysis give weights to the features accordingly, so feature scaling may not have much effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c1b33",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://towardsai.net/p/data-science/scaling-vs-normalizing-data-5c3514887a84\n",
    "\n",
    "https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
